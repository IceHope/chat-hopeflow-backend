{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\.venv\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\iceHope\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\iceHope\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 76, in generate\n    else await self._manual_json(input, **{**kwargs, \"name\": call_name})\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 133, in _manual_json\n    json = try_parse_json_object(output)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n             ^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\iceHope\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\iceHope\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\iceHope\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\.venv\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\iceHope\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\iceHope\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 76, in generate\n    else await self._manual_json(input, **{**kwargs, \"name\": call_name})\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 133, in _manual_json\n    json = try_parse_json_object(output)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n             ^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\iceHope\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\iceHope\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\iceHope\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\.venv\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\iceHope\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\iceHope\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 76, in generate\n    else await self._manual_json(input, **{**kwargs, \"name\": call_name})\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 133, in _manual_json\n    json = try_parse_json_object(output)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n             ^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\iceHope\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\iceHope\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\iceHope\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\.venv\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\iceHope\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\iceHope\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 76, in generate\n    else await self._manual_json(input, **{**kwargs, \"name\": call_name})\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 133, in _manual_json\n    json = try_parse_json_object(output)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n             ^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\iceHope\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\iceHope\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\iceHope\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\.venv\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\iceHope\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\iceHope\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 76, in generate\n    else await self._manual_json(input, **{**kwargs, \"name\": call_name})\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 133, in _manual_json\n    json = try_parse_json_object(output)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Git\\Open\\graphrag-local-ollama\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n             ^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\iceHope\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\iceHope\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\iceHope\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
