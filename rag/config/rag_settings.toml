[rag]
name = "hope_rag"
version = "0.1.1"

[rag.milvus]
uri = "http://localhost:19530"
collection_admin = "hope_admin"
collection_guest = "hope_guest"
collection_test = "hope_test"

[rag.retriver]
similarity_top_k = 6
rerank_top_n = 3

[rag.embedding.bge]
source = "bge-small-zh-v1.5"
embed_batch_size = 10

[rag.embedding.bge-m3]
file_path = "F:/HuggingFace/Embedding/bge-m3"

[rag.multi_modal]
source = "gemini-1.5-flash"

[rag.evaluation]
retrieval = ["mrr", "hit_rate"]
response_label = true
response = ["Faithfulness", "Answer Relevancy", "Correctness", "Semantic Similarity"]

[rag.index]
persist_path = "localdata/storage"
vector_store.type = "FAISS"

# llm configurations, source support API: OpenAI,DashScope or PAI-EAS's deployment
# eg.
# source = "PaiEas"
# endpoint = ""
# token = ""
[rag.llm]
source = "DashScope"
name = "qwen-turbo"

[rag.llm.multi_modal]
source = "DashScope"

[rag.node_parser]
type = "Sentence"
chunk_size = 500
chunk_overlap = 10

[rag.postprocessor]
reranker_type = "simple-weighted-reranker" # [simple-weighted-reranker, model-based-reranker]
reranker_model = "bge-reranker-base" # [bge-reranker-base, bge-reranker-large]
keyword_weight = 0.3
vector_weight = 0.7
similarity_threshold = 0.5
top_n = 2

[rag.query_engine]
type = "RetrieverQueryEngine"

[rag.retriever]
similarity_top_k = 3
retrieval_mode = "hybrid" # [hybrid, embedding, keyword, router]
query_rewrite_n = 1 # set to 1 to disable query generation

[rag.synthesizer]
type = "SimpleSummarize"
text_qa_template = "参考内容信息如下\n---------------------\n{context_str}\n---------------------根据提供内容而非其他知识回答问题.\n问题: {query_str}\n答案: \n"
